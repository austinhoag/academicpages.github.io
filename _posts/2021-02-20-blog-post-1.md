---
title: 'Detecting craters on Mars with a Convolutional Neural Network'
date: 2021-02-20
permalink: /posts/2021/02/blog-post-1/
tags:
  - machine-learning
  - neural-networks
  - mars
---

<style>
* {
  box-sizing: border-box;
}

p.my-caption {
	font-size: 14px;
}
.column {
  float: left;
  width: 33.33%;
  padding: 5px;
}

/* Clearfix (clear floats) */
.row::after {
  content: "";
  clear: both;
  display: table;
  text-align: center;
}
</style>

# Background 
The successful landing of NASA's Perserverance rover yesterday marks the fifth chapter in NASA's rover program, which began sending rovers to Mars in the late 1990s. None of these missions would have been possible without the Mars orbiters, which NASA and the Soviet Union (and ESA, more recently) started sending as far back as the 1970s. The orbiters are used to map the surface to find optimal sites for the rovers to land and then explore. They  also communicate with the rovers during their missions to help them navigate and send and receive data to and from Earth. While the orbiters could be seen as there to faciliate the rover missions, they are also interesting in their own right. They generate the most detailed images that humanity has captured of another planet. Until I started looking at these images, I really did not appreciate how diverse and rich the Martian surface is. 

The <a href="https://en.wikipedia.org/wiki/Mars_Reconnaissance_Orbiter">Mars Reconnaissance Orbiter</a> contains HiRISE (High Resolution Imaging Science Experiment), the "most powerful camera ever sent to another planet," according to the <a href="https://www.uahirise.org/epo/about/">instrument homepage</a>. This camera takes images of Mars at extremely high resolution (< 1 meter/pixel) and has been taking data since 2006. As a result, this has generated an enormous amount of high quality image data, and the HiRISE team has been kind enough to make a lot of it public. Based on the variety of surface features present in the images and the size of the dataset, I thought it would make an interesting and challenging dataset for feature detection with a convolutional neural network (CNN).

<div style="text-align: center">
	<div class="row">
		<div class="column">
			<img src="/images/ESP_040776_2115.jpg" width="400" height="300">
		</div>
		<div class="column">
			<img src="/images/ESP_033165_2195.jpg" width="400" height="300">
		</div>
		<div class="column">
			<img src="/images/ESP_021496_1255.jpg" width="400" height="300">
		</div>
	</div>
<div class="row">
	<div class="column">
		<img src="/images/ESP_016978_1730.jpg" width="400" height="300">
	</div>
	<div class="column">
		<img src="/images/ESP_016641_2500.jpg" width="400" height="300">
	</div>
	<div class="column">
		<img src="/images/ESP_023464_0945.jpg" width="400" height="300">
	</div>
</div>
	<p class="my-caption"> Images of the Martian surface taken by the HiRISE camera on NASA's Mars Reconnaissance Orbiter </p>
</div>

# Setting a goal
My ultimate goal was to train a CNN to detect various surface features such as dunes, craters, polar ice, ancient river deltas, and others in each of the images. This type of problem is considered a multi-class classification problem in the language of machine learning. Such problems are more complicated and require more total training data than single-class classification problems, such as determining whether an image is of a dog or a cat. I figured that if I could not get good performance on a single-class classification problem given this dataset, then I would have no hope on the multi-class problem. 

So I set an intermediate goal to train a CNN to classify whether an image had crater(s) in it. Craters are mostly very circular and they are in enough of the images to make a relatively large dataset for training, validating and testing the model. My goal was to assemble a dataset of 2400 images total, with 1200 images for each class (crater or no crater). This is on the small side for training, validation and testing a CNN, but it is large enough to determine whether the model could start to learn. The <a href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR-10</a> dataset, for example, contains 6000 images for each object class (e.g truck, airplane, automobile). 

# The dataset
HiRISE provides \~2000 high-resolution RGB images of Mars with metadata and about 66,000 (at the time of writing this) additional images without metadata. The images were taken from all over the surface of the planet and at many different times of sol (the name for a day on Mars). The metadata capture these parameters, so I thought it would be interesting to look at model performance as a function of these parameters. The notorious dust storms that make seeing the surface <a href="https://en.wikipedia.org/wiki/Timeline_of_Opportunity_(rover)#Dust_storm">(and communicating with the rovers)</a> difficult are absent from these images. Mars' atmosphere is very thin compared to Earth's so clouds do not affect the image quality as they often do in Earth satellite imagery. Finally, the images are taken at approximately the same altitude, so the scene size (and pixel size) is similar among all of the images. 

I downloaded the \~2000 images that had metadata from here: <a href="https://static.uahirise.org/images/wallpaper/">https://static.uahirise.org/images/wallpaper/</a>. The images are provided in various resolutions (but all of the same scene size). Because I planned to downsampled the images to a lower resolution than their lowest resolution offered, I chose their lowest resolution option, 800x600 pixels. I scraped the metadata from their API (<a href="https://hirise.lpl.arizona.edu/">https://hirise.lpl.arizona.edu/</a>) using beautiful soup.

Unfortunately, after looking at a bunch of the images I noticed that only about 10% of the images had craters in them. This meant that I was likely to not meet my goal of labeling 1200 images with craters for training/validation/testing. Fortunately, the scenes are large and often contain multiple craters per image when craters are present. I was able to chunk each image up into 12 square sub-images, resulting in a total of \~24000 200x200 images. About 5% of these smaller images contained craters, so this enabled me to just barely meet my goal. 

# Making the training set
 
While the image metadata contain a text description of the scene, the description is not sufficient for labeling whether an image has a crater in it. This becomes especially true after dividing each original image into 12 sub-images. As a result, I decided to make the training set manually by visually inspecting the images. I figured this was the only way to control the quality of the training/validation/test sets. High quality input to the model is essential for the model to perform well.

I created a flask app to make inspecting and labeling the images as quick and painless as possible. 

<div style="text-align: center">
	<img src="/images/flask_app.png" width="100%">
	<p class="my-caption"> A screenshot from the flask application I developed to quickly label images with craters in them for training the CNN</p>
</div>

I set up the application so that clicking anywhere in the image would select a checkbox indicating whether the image contained a crater. Each page is an html form that when submitted updates a CSV file to record the image labels. I paginated the display so that each page had 100 images. This allowed for fast page loading and form submission times. I was able to look through a page of 100 images and label them in under a minute, so the whole process ended up taking me a few hours one afternoon. 

Craters come in a wide range of sizes, but they are primarily circular. In deciding on which images contained craters, I had to make a few choices which I knew would influence how the model learned and ultimately performed. On my first pass through the data, I decided to label anything that I though belonged to a crater of any size. This included very small craters and craters that were so large that only a fraction of them was visible on the screen. 

<div style="text-align: center">
	<div class="row">
		<div class="column">
			<img src="/images/ESP_020922_1635_x600-800_y0-200.jpg" width="200" height="200">
		</div>
		<div class="column">
			<img src="/images/ESP_016173_2005_x0-200_y200-400.jpg" width="200" height="200">
		</div>
		<div class="column">
			<img src="/images/PSP_007808_1575_x0-200_y400-600.jpg" width="200" height="200">
		</div>
	</div>
	<p class="my-caption"> Left: An image with several obvious, medium sized craters. Middle: a portion of a very large crater wall. Right: several very small craters. In my initial training/val/test sets, I labeled all of these images as having craters. In my second pass, I only used the left image.</p>
</div>

As I will show, this ended up being over optimistic. A smaller training set with a stricter definition of a crater ended up outperforming my initially trained model.

# Choosing a model
I tried several models, starting with a very simple CNN architecture consisting of only a few hidden layers. I found that such models were unable to reach over 60% accuracy, regardless of how strict my training set was. I decided to look into established architechtures and settled the Xception model. This model performs extremely well on the ImageNet dataset as well as the CiFAR dataset, so I decided to try it on my dataset. The architecture is shown below:

<div style="text-align: center">	
	<img src="/images/xception.jpg" width="100%" >
	<p class="my-caption"> The Xception network (Chollet et al. 2017)</p>
</div>

This is a state of the art CNN model, which is an evolution from the Inception-style architecture. I implemented it with Keras/Tensorflow. My implemented model contains a total of 2,782,649 parameters, with 2,773,913 trainable parameters. 

# Training the model

I split the \~2400 images in my labeled dataset into \~1600 for training, \~400 for validation and \~400 for testing. I also downsampled the images from 200x200 to 180x180 pixels since that is the input size for my CNN.

<div style="text-align: center">	
	<img src="/images/training_set_example.png" width="100%" >
	<p class="my-caption"> Examples from my first pass training and validation sets</p>
</div>

I used a batch size of 32 and trained the model over 60 epochs. 

<div style="text-align: center">	
	<img src="/images/training_attempt1.png" width="75%" >
	<p class="my-caption"> Accuracy of training and validation sets of the Xception model over 60 epochs of training using my first-pass training set.</p>
</div>

The accuracy of the validation set increased until about 40 epochs. After that it flattened while the training accuracy still increased -- a tell-tale sign of overfitting. The maximum validation set accuracy is 83% at epoch 53, so I chose this epoch as my best-fit model. The noisiness in the validation accuracy curve is a clear indication that especially my validation set of 400 images is on the small side. Still, the general trend of the model is clear. 

## Model performance 
Using the best-fit model, the I found a test-set accuracy of 78%. This is not too bad, especially given the limited number of training examples, but could definitely be improved. 

To get more insight into where the model could use improvement, it is useful to look at the confusion matrix:

<div style="text-align: center">	
	<img src="/images/confusion_matrix_attempt1.png" width="75%" >
	<p class="my-caption"> Confusion matrix of Xception model trained using first-pass training set. Each cell contains the total number of predictions in the category followed by the fraction in of predictions in that row. </p>
</div>

The model has good precision but not very good recall. In other words, the model can pick out when an image does NOT contain a crater much better than when an image contains a crater. In an analogy with testing for the presence of a virus (where 0=no virus and 1=virus), if a test had our model's confusion matrix told you that you had the virus you could be 89% sure it was correct. However, if it told you you didn't have the virus, you could only be 66% sure it was correct. When put that way, it is clear this is far from a perfect model. Let's look at some image examples of false negatives, i.e. where the model predicted no craters but there were in fact craters:

<div style="text-align: center">	
	<img src="/images/false_negatives_attempt1.png" width="75%" >
	<p class="my-caption"> Nine example test-set images where the model predicted no crater but the image was labeled as having a crater(s) (from the first-pass dataset). The probability that the model predicted that the image contained craters is listed above each image. </p>
</div>

In a few of these images, such as images A, C and G, the craters are not obvious even to the human eye, especially at this resolution. In image H, the crater is only partially visible. Probabilities close to 0.5, as in images B, C, F and G indicates that the model was ambiguous about these images. At this point I decided that if I wanted a model with better performance, a good idea would be to clean up the dataset that I was using for training, validating and testing the model. I worried that dropping too many images would make my small dataset even smaller, but it was a test worth doing.

## Re-modeling with a cleaner dataset
I went back through the train/val/test datasets and removed labeled images from either category that had partial or inobvious craters. Fortunately I had not exhausted the entire dataset in my first pass, so I had new images in which to find obvious craters. This partially balanced out the fact that I removed a significant number of images with inobvious craters. I made sure that none of the images that I discarded as being too inobvious ended up in the no crater class of the cleaner dataset. I ended up with a cleaned dataset consisting of 2100 images, which I split into 1500 for training, 300 for validation and 300 for testing. This was smaller than I had hoped, but it would at least allow me to test whether dumping the dirty data would improve the model, as I expected.

I retrained the same Xception model on the cleaner dataset using the same batch size (32) and number of epochs (6) as before. 

<div style="text-align: center">	
	<img src="/images/training_attempt2.png" width="75%" >
	<p class="my-caption"> Accuracy of training and validation sets of the Xception model over 60 epochs of training using my cleaner dataset.</p>
</div>

The result was significantly (>\~10%) better than the first pass on the validation set, despite having a 12.5% smaller dataset. While I was not surprised to see an improvement, the magnitude of the improvement was very encouraging. The accuracy of the validation set increased on pace with the training accuracy for all epochs. This indicates that overfitting never became an issue. The maximum validation set accuracy of 92% came on the final epoch, 60, so I chose this epoch as my best-fit model. The noisiness in the validation accuracy curve is a clear indication that especially my validation set of 400 images is on the small side. Still, the general trend of the model is clear. 

## Model performance (cleaner dataset)
Using the best-fit model from epoch 60, I found a test-set accuracy of 89%. This is much better than I was expecting starting out with project and encouraged me that I could at least attempt multi-class classification on these images. This also underscores the maxim that "better" data is better than more data. The classes are balanced in this dataset, so accuracy is not a biased measure of model performance. Still, accuracy is a blunt measurement and I prefer the ROC curve and confusion matrix for model comparison. 


<div style="text-align: center">	
	<img src="/images/roc_auc_compare1and2.png" width="75%" >
	<p class="my-caption"> ROC curves for the two Xception models using different datasets compared to the baseline of randomly guessing class membership. The green curve is the Xception model I ran using the larger, first-pass dataset and the red curve is the same model ran on the 12% smaller dataset but with much cleaner data.   </p>
</div>

As with the accuracy, the ROC curve shows the clear improvement in performance after cleaning up the dataset. The ROC area-under-curve (ROC AUC) score for the green curve (first-pass dataset) is 0.87 whereas for the green curve (cleaned dataset) is 0.95, a very good score -- A perfect model has a score of 1.0. Finally, we can examine the confusion matrix for the better model to see where it could be improved:

<div style="text-align: center">	
	<img src="/images/confusion_matrix_attempt2.png" width="75%" >
	<p class="my-caption"> Confusion matrix of Xception model trained using cleaned (second-pass) training set. Each cell contains the total number of predictions in the category followed by the fraction in of predictions in that row. </p>
</div>

As with the previous model, the model has much better precision than recall. It still struggles to detect all of the images with craters. Let's look at some examples where this happened, i.e. false negatives:

<div style="text-align: center">	
	<img src="/images/false_negatives_attempt2.png" width="75%" >
	<p class="my-caption"> Nine example test-set images where the predicted no crater but the image was labeled as having a crater(s) (from the model run on the cleaned dataset). The probability that the model predicted that the image contained craters is listed above each image. </p>
</div>

Images B, D, E, G and I are all incomplete craters which I failed to remove from my dataset, so I suspect that is at least part of why they are not predicted as craters. The craters in A, F and H have low contrast compared to the background of their image, so that could explain why they are false negatives. 

## Further analyses

TBC...